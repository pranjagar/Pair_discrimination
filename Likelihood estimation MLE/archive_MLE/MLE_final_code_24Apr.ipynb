{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "import cmath\n",
    "import sympy as sym\n",
    "import scipy as sci\n",
    "import matplotlib.ticker as ticker\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import cmath as cm\n",
    "from IPython.display import display, Latex\n",
    "from sympy import pprint\n",
    "from scipy.optimize import minimize as GDlib\n",
    "import MLE_functions as fn\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.optimize import least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0000000000000013, [2.265296877986555, 1.8918211384912382, 1.3916056580056657, 0.5945610256119627, 0.6667857953922188]), (1.0000000000000013, [2.3177043743113135, 1.0861105600070926, 1.4171994435824677, 2.7598348605383687, 1.4717224455442677]), (1.0000000000000016, [1.0533169989245574, 2.3794407509967326, 2.2734063354534615, 1.6308158858652484, 2.0948104537749885]), (1.0000000000000016, [2.6692081392213094, 0.43303405565779046, 0.2544337544362274, 2.538710432846454, 0.9381217268962634]), (1.0000000000000018, [0.008143607094407947, 0.8395944110464606, 1.2494890890600456, 1.224722634865263, 0.2181002179750774]), (1.0000000000000018, [0.19710741559889922, 2.8758734268985156, 2.3147466538819654, 1.090173809309891, 2.4741186138400684]), (1.0000000000000018, [0.36892030171070755, 1.9823010590846581, 2.547536588552738, 1.0245634347111645, 2.307305078649268]), (1.0000000000000018, [0.7673745811328441, 0.8319143622836367, 0.7751357484730328, 1.8200063217718792, 1.4797335759610222]), (1.0000000000000018, [0.8200070442010029, 0.41851427234142957, 0.395417936231743, 2.0586384712407146, 1.537795553068602]), (1.0000000000000018, [1.4268634145425636, 1.732116924154014, 2.9788395755837915, 1.7092998164010638, 1.7491839520874568]), (1.0000000000000018, [1.4798919229452234, 1.7160352474502027, 2.1070184578683318, 1.0077201937265985, 2.9163960055297675]), (1.0000000000000018, [1.5001193382816829, 1.268135116726433, 2.310409712629743, 2.0795432449174798, 3.0879340830810884]), (1.0000000000000018, [1.835449255191606, 0.4832687990488434, 0.9649839220721494, 2.5781007347657465, 1.4340894619146278]), (1.0000000000000018, [1.8438973076392058, 0.055852285865742216, 0.9919821781826097, 0.5817576680248613, 1.188346779064289]), (1.0000000000000018, [1.961504504069155, 2.9642848375240733, 0.3734752310447801, 2.9791024507967507, 2.4614975565968673]), (1.0000000000000018, [2.452948634727891, 1.2423957602192466, 0.4543046716075523, 2.920008782440968, 3.1102830782411615]), (1.0000000000000018, [2.847193924332222, 0.39791439671111184, 1.9719438325148755, 2.444417084236547, 0.8945025706074199]), (1.000000000000002, [0.05833162791925264, 0.59391607837943, 2.012834049816301, 2.530405760446775, 2.557344702994141]), (1.000000000000002, [1.3289147779035264, 0.18499636999374441, 0.10626868184598506, 3.059159316169271, 1.2095716837768675]), (1.000000000000002, [1.4244205055880774, 0.8689295748467457, 1.303650285854331, 0.9740066639027471, 3.074323294326377]), (1.000000000000002, [1.5001707178832728, 1.7526687672255763, 2.2192720153529177, 1.3368955973394787, 2.1753464576820503]), (1.000000000000002, [1.506033321696437, 3.09401935996995, 2.1709716099738166, 0.9662937750034374, 3.0714859023672023]), (1.000000000000002, [1.6640619109307182, 0.5520432119581142, 2.334725349040252, 1.4723805487319994, 0.9604514550581605]), (1.000000000000002, [1.7524482034386006, 3.0672965423624174, 0.9327404803538035, 2.2584452945865325, 2.240806539215812]), (1.000000000000002, [2.5672022180173206, 2.4569962319535867, 0.3968541291873788, 2.808860416664631, 0.3644178054755481]), (1.000000000000002, [3.0255053481571816, 0.49862626682457317, 1.2348807999853195, 2.8905902461543636, 0.30449564738499674]), (1.0000000000000022, [0.0630685635474806, 1.9368233539316748, 0.5983913929161399, 1.7259503227967061, 1.9155890725598803]), (1.0000000000000022, [0.2938754276168328, 2.2409180911348088, 0.055916890441774454, 0.2526694235288669, 2.2102957187846126]), (1.0000000000000022, [0.9049041334302024, 1.601950458741814, 1.0814756160135774, 1.6704475035467705, 1.4792934818936372]), (1.0000000000000022, [1.9087642475707316, 0.6454503344411537, 2.062999847211206, 1.5068733461550967, 0.5465972860037046]), (1.0000000000000022, [2.1441311845781126, 2.8141617094739515, 1.4868394328631356, 2.288044624290603, 0.9809117047557843]), (1.0000000000000022, [3.0254480047466794, 2.959477870507001, 2.873801154866969, 2.9120834891935896, 1.2394566529023419]), (1.0000000000000024, [0.2196520423476611, 1.4619190137608329, 1.9408848968062358, 0.9109494282725815, 1.892727569707614]), (1.0000000000000024, [1.178418270384597, 0.5280596939391728, 2.025606545568277, 1.1122477330484646, 1.26030207993734]), (1.0000000000000024, [1.3356824394871845, 1.8269516427876746, 0.995174688920182, 2.9650039867422637, 0.171383227412481]), (1.0000000000000024, [1.6004538508274346, 0.31239854983432297, 1.5131177349480238, 0.6713119878321041, 0.9678536771702639]), (1.0000000000000024, [1.680879470713279, 0.32180921652347766, 0.14533775375956998, 1.0314503125204477, 1.9218013404441907]), (1.0000000000000024, [1.8291979857413954, 2.5758089652958454, 2.0880828942974006, 2.064452164774298, 1.9934432629156682]), (1.0000000000000024, [2.927139259956765, 1.8932919590548203, 2.1882022298435277, 2.6392076757665888, 0.6464135628747527]), (1.0000000000000024, [2.9708958023722247, 2.1795483408229246, 3.0514508564452965, 1.0539329300610907, 2.846694358574569]), (1.0000000000000027, [0.853486794908298, 1.5860952017484184, 0.31780130458344286, 2.692985011948472, 2.128325463449944]), (1.0000000000000027, [2.5728718000477424, 2.6057924572540867, 1.8656503125367807, 2.5176451475810686, 0.7713891071157736]), (1.0000000000000029, [0.06976299086487053, 1.9927254149148972, 0.5038574281960438, 2.1055445394917913, 1.9638451415817062]), (1.0000000000000029, [1.5563662766269575, 0.38380844584465795, 2.3721031526652534, 0.0434317299112015, 2.2796357241744434]), (1.0000000000000029, [1.901290711733295, 2.1104410583281337, 2.244534872211415, 0.906159204389324, 2.9585300204818434]), (1.0000000000000033, [0.7794539532110075, 2.2731774047404976, 1.0178756627551444, 1.6886351753995192, 0.28436317458474725]), (1.0000000000000033, [2.3350260926373925, 0.37838431612175233, 2.713287243375201, 0.07955278413663479, 2.062968976157762]), (1.0000000000000033, [2.4024131346292887, 0.9407154137247365, 2.2980640291067465, 0.9038736782315469, 1.9001445417655487]), (1.0000000000000033, [2.53079689495261, 1.8081182074684166, 2.9150316312788176, 0.8639647675437065, 0.570884430487936]), (1.0000000000000047, [2.3844865263485504, 3.0158157846746017, 2.944002838228662, 0.13166862266691434, 0.20470391951076436])]\n"
     ]
    }
   ],
   "source": [
    "pr_num = [sym.lambdify((fn.theta1, fn.theta2, fn.phi1, fn.phi2, fn.alpha), i) for i in fn.pr]   # lambdify the probabilities\n",
    "\n",
    "# checking probabilities syum to 1\n",
    "# creating fucntion to check at 100 random points the sum of the probs to see if any of them dont sum to 1\n",
    "def check_sum(xx):\n",
    "    sum_pr_list = []\n",
    "    params_list = []\n",
    "    for i in range(xx):\n",
    "        theta1 = rand.uniform(0, m.pi)\n",
    "        theta2 = rand.uniform(0, m.pi)\n",
    "        phi1 = rand.uniform(0, m.pi)\n",
    "        phi2 = rand.uniform(0, m.pi)\n",
    "        alpha = rand.uniform(0, m.pi)\n",
    "        params = [theta1, theta2, phi1, phi2, alpha]\n",
    "        sum_pr = sum([i(theta1, theta2, phi1, phi2, alpha) for i in pr_num])\n",
    "        \n",
    "        params_list.append(params)\n",
    "        sum_pr_list.append(sum_pr)\n",
    "        \n",
    "        #zip and sort in the order of sum of the probabilities\n",
    "        zipped_lists = zip(sum_pr_list, params_list)\n",
    "        sorted_zipped_lists = sorted(zipped_lists)\n",
    "        \n",
    "    return sorted_zipped_lists\n",
    "\n",
    "sorted_zipped_lists = check_sum(50)\n",
    "\n",
    "print(sorted_zipped_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving the previous loss function, new code: \n",
    "def loss_new(vars, params_tru = [0, m.pi/2, 0, 0, m.pi/4], N = 10000, nj_unnorm = None, lossfn = 'new'):                         \n",
    "    if nj_unnorm != None:\n",
    "        n_unnorm = nj_unnorm\n",
    "    else:\n",
    "        num_expt = fn.num_experiment(params= params_tru, N = N)                             # for getting the collapse count\n",
    "        n_unnorm = num_expt[4]\n",
    "\n",
    "    n = [i/sum(n_unnorm) for i in n_unnorm]                                                # normalise the counts\n",
    "    eq_pr = [pr_num[i](*vars) for i in range(len(pr_num))]\n",
    "    \n",
    "    lnl_p = [i * np.log(p) if i != 0 else 0 for i, p in zip(n, eq_pr)]                          # manually putting zero for cases of p_i*log(n_i) , if any n_i are 0.\n",
    "    lnl_n = [i * np.log(i) if i != 0 else 0 for i in n]                         \n",
    "    del_lnl = [(k-l) for k,l in zip(lnl_p, lnl_n)]\n",
    "    loss = sum([abs(i) for i in del_lnl])\n",
    "\n",
    "    if lossfn == 'old':\n",
    "        loss = sum([abs(i) for i in lnl_p])\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the gradient function\n",
    "def grad(vars, params_i = None ,nj_unnorm = None, s=(1/500), N = 10000, loss = 'new'):    # \"point\" is the list of parameters\n",
    "    if nj_unnorm != None:\n",
    "        n_unnorm = nj_unnorm\n",
    "    else:\n",
    "        num_expt = fn.num_experiment(params= params_i, N = N)              # for getting the collapse count\n",
    "        n_unnorm = num_expt[4]\n",
    "    \n",
    "    del_theta1 = loss_new(vars = [vars[0]+s,vars[1],vars[2],vars[3],vars[4]], nj_unnorm= nj_unnorm, lossfn = loss) - loss_new(vars = [vars[0],vars[1],vars[2],vars[3],vars[4]], nj_unnorm= nj_unnorm, lossfn = loss)\n",
    "    del_theta2 = loss_new(vars = [vars[0],vars[1]+s,vars[2],vars[3],vars[4]], nj_unnorm= nj_unnorm, lossfn = loss) - loss_new(vars = [vars[0],vars[1],vars[2],vars[3],vars[4]], nj_unnorm= nj_unnorm, lossfn = loss)\n",
    "    del_phi1   = loss_new(vars = [vars[0],vars[1],vars[2]+s,vars[3],vars[4]], nj_unnorm= nj_unnorm, lossfn = loss) - loss_new(vars = [vars[0],vars[1],vars[2],vars[3],vars[4]], nj_unnorm= nj_unnorm, lossfn = loss)\n",
    "    del_phi2   = loss_new(vars = [vars[0],vars[1],vars[2],vars[3]+s,vars[4]], nj_unnorm= nj_unnorm, lossfn = loss) - loss_new(vars = [vars[0],vars[1],vars[2],vars[3],vars[4]], nj_unnorm= nj_unnorm, lossfn = loss)\n",
    "    del_alpha  = loss_new(vars = [vars[0],vars[1],vars[2],vars[3],vars[4]+s], nj_unnorm= nj_unnorm, lossfn = loss) - loss_new(vars = [vars[0],vars[1],vars[2],vars[3],vars[4]], nj_unnorm= nj_unnorm, lossfn = loss)\n",
    "    \n",
    "    grad = [del_theta1/s,del_theta2/s,del_phi1/s,del_phi2/s,del_alpha/s]\n",
    "    grad = [i if abs(i) > 1e-12 else 0 for i in grad]\n",
    "    \n",
    "    xx = 5\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original working GD function\n",
    "def GD_new(params_i, params_true = [0, m.pi/2, 0, 0, m.pi/4], nj_unnorm = None, nit = 20, step = 1/120, N = 10000, loss = 'new'):\n",
    "    if nj_unnorm == None:                                                           # priority is using the given collapses, else build using fn.num_expt\n",
    "        nj_unnorm = fn.num_experiment(params= params_true, N = N, an_pr= True)[4]           \n",
    "    params = params_i\n",
    "    loss_i = loss_new(vars = params, nj_unnorm= nj_unnorm)                                          # Initial parameters, loss\n",
    "    \n",
    "    params_history = [params]                                          # To store the history of parameters, loss and steps\n",
    "    loss_history = [loss_i]                                         \n",
    "    steps = [step]\n",
    "    for i in range(nit):\n",
    "        if loss == 'new':\n",
    "            gradients = grad(vars = params, nj_unnorm= nj_unnorm)\n",
    "        else:\n",
    "            gradients = grad(vars = params, nj_unnorm= nj_unnorm, loss = 'old')\n",
    "        params = [(i - step*grad) for i, grad in zip(params, gradients)]    # Basically, same as entry by entry updating the parameters.\n",
    "        \n",
    "        if i == (.5*nit)//1:                                   # variable step size after 50% and 75% of the iterations\n",
    "            step = step/2\n",
    "        if i == (.75*nit)//1:\n",
    "            step = step/5\n",
    "        steps.append(step)\n",
    "        params_history.append(params)\n",
    "        loss_history.append(loss_new(vars = params, nj_unnorm= nj_unnorm))\n",
    "        if sum(abs(m-n) for m,n in zip(params, params_history[-2])) < 1e-10:\n",
    "            print(f'Gradient descent converged at {params} after {i} iterations')\n",
    "    xx = 6                                                          # dummy for debugging\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a randomized function for pure gradient descent without using the inversion. The idea is that this function will take true parameters, number of iterations, number of random trials, etc. as arguments and it will basically choose randomly that many number of lists and then run the previously defined gradient descent function gd underscore new on each of those and add the resultant parameters and the corresponding fidelities to lists and then it will pick out the one with the best fidelity and the corresponding parameters and return them\n",
    "def gd_rand(params_tru = [0, m.pi/2, 0, 0, m.pi/4], N = 10000, nj_unnorm = None, loss = 'new', step = 1/3, nit = 100, rat = 30):\n",
    "    params_list = []\n",
    "    fid_list = []\n",
    "    for i in range(rat):\n",
    "        params_i = [rand.uniform(0,m.pi) for i in range(5)]\n",
    "        opt_params_i = GD_new(params_i = params_i, params_true = params_tru, N = N, nj_unnorm = nj_unnorm, loss = loss, nit = nit, step = step)\n",
    "        fid_list.append(fn.fid(opt_params_i, params_tru))\n",
    "        params_list.append(opt_params_i)\n",
    "\n",
    "    # zip the lists\n",
    "    results_list = zip(fid_list, params_list)\n",
    "    \n",
    "    # sort and pick the best ten percent results\n",
    "    results_list = sorted(results_list, key = lambda x: x[0], reverse = True)\n",
    "    best_results = results_list[:rat//10]                       # best 10% of the fidelities and the corresponding parameters\n",
    "    \n",
    "    best_params = [i[1] for i in best_results]\n",
    "    best_fids = [i[0] for i in best_results]\n",
    "\n",
    "    return (best_params, best_fids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed it the true parameters, it will give results comparing the parameters and the fidelities of the inversion method and the hybrid method\n",
    "def results(params_tru, nit = 100, step = 1/70, N = 10000):\n",
    "    inv = fn.Inversion_new(params = params_tru, N = N, threshold = 'variable')\n",
    "    nj_unnorm = inv[6]\n",
    "    opt_inv = inv[2]\n",
    "    # inv_fid_c = inv[0]                              # using coeffcients from the inversion protocol to calculate fidelity\n",
    "    fid_inv = fn.fid(opt_inv, params_tru)        # using parameters to calculate fidelity, should be same as from the coefficients\n",
    "    \n",
    "    opt_GD = GD_new( params_i = opt_inv , nj_unnorm = nj_unnorm, nit = nit, step = step, N = N)\n",
    "    fid_opt = fn.fid(opt_GD, params_tru)\n",
    "    \n",
    "    ppm_inv_fid = [(1- i)*1e6 for i in fid_inv]     # parts per million error in fids\n",
    "    ppm_opt_fid = [(1- i)*1e6 for i in fid_opt]\n",
    "    \n",
    "    # print(f'tru_params_deg: {[i*(180/m.pi) for i in params_tru]} \\n tru_params: {params_tru} \\n opt_inv: {opt_inv} \\n opt_GD: {opt_GD} \\n \\n  inv_fid: {fid_inv} \\n GD_fid: {fid_opt}  \\n\\n ppm_inv_fid: {ppm_inv_fid} \\n ppm_opt_fid: {ppm_opt_fid} \\n')\n",
    "\n",
    "    return [params_tru, nj_unnorm, opt_inv, opt_GD , fid_inv, fid_opt, ppm_inv_fid, ppm_opt_fid]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tru params  [45.0, 135.0, 0.0, 0.0, 45.0] \n",
      " opt_p_degrees: [360.0178561892273, 89.71741720969744, 124.52625583597869, -154.54877062635077, (44.59358455611585-7.67504195354202e-17j)] \n",
      " params_i_degrees: [45.413971053989975, 44.97208496779245, 0.5324424926602901, -179.92245596910786, (44.65140134482599-7.67504195354202e-17j)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.49982336222981805, 0.5044532842364494],\n",
       " [0.9999262110315976, 0.9999993047054998])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying the old loss, now wwith the correct probabilities\n",
    "\n",
    "# params_i = [rand.uniform(0,m.pi) for i in range(5)]\n",
    "params_i = fn.Inversion_new(params = [m.pi/4, m.pi/2+m.pi/4, 0, 0, m.pi/4], N = 10000, threshold = 'variable')[2]\n",
    "params_tru = [m.pi/4, m.pi/2+m.pi/4, 0, 0, m.pi/4]\n",
    "# opt_p = GD_new(params_i = params_i, params_true = params_tru, N = 10000, loss = 'old', nit = 50, step = 1/5)\n",
    "opt_p = GD_new(params_i = params_i, N = 10000, nit = 50, step = 1/5)\n",
    "\n",
    "params_i_degrees = [i*(180/m.pi) for i in params_i] \n",
    "opt_p_degrees = [i*(180/m.pi) for i in opt_p]\n",
    "\n",
    "print('\\n tru params ', [i*(180/m.pi) for i in params_tru] ,'\\n opt_p_degrees:', opt_p_degrees, '\\n params_i_degrees:', params_i_degrees)\n",
    "\n",
    "fids_opt = fn.fid(opt_p, params_tru)\n",
    "fids_inv = fn.fid(params_i, params_tru)\n",
    "fids_opt, fids_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying another way simplifying the laws at many many many random points and picking the ones in which the loss is really close to the theoretical loss and then refining by making steps smaller around each of those points until we reach the threshold precision let's say 1e-5 in the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tru_params_deg: [29.999999999999996, 119.99999999999999, 45.0, 90.0, 29.999999999999996] \n",
      " tru_params: [0.5235987755982988, 2.0943951023931953, 0.7853981633974483, 1.5707963267948966, 0.5235987755982988] \n",
      " opt_inv: [0.5257531629285364, 1.012673773364041, 0.7943980307830147, -1.5594632130249584, (0.5353590075198238+2.1985963773791503e-17j)] \n",
      " opt_GD: [0.5278778329628031, 1.0283658526741655, 0.8039305470572345, -1.5573297830992687, (0.525432692914733+2.1985963773791503e-17j)] \n",
      " \n",
      "  inv_fid: [0.9999801340334321, 0.9987835982022493] \n",
      " GD_fid: [0.9999169790745976, 0.9996106917634611]  \n",
      "\n",
      " ppm_inv_fid: [19.86596656788908, 1216.4017977507324] \n",
      " ppm_opt_fid: [83.02092540235506, 389.3082365389322] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#try\n",
    "params_tru1 = [m.pi/6, m.pi/2+m.pi/6, m.pi/4, m.pi/2, m.pi/6]\n",
    "results(params_tru1, nit = 300, step = 1/200, N = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtu =  [0.5235987755982988, 2.0943951023931953, 0.7853981633974483, 1.5707963267948966, 0.5235987755982988]\n",
      "nj_unnorm =  [654, 558, 1169, 967, 2313, 521, 1260, 1152, 1406]\n",
      "opt_inv =  [0.5254370610538157, 1.030984356143689, 0.7670544953445808, -1.6068690955721805, (0.5346630644404312+2.5302074683784063e-17j)]\n",
      "opt_GD =  [0.5320294066145164, 1.0327066070674442, 0.7689496798867287, -1.6064174644839841, (0.5366920641304006+2.5302074683784063e-17j)]\n",
      "fid_inv =  [0.9999333970821788, 0.9994887602573697]\n",
      "fid_GD =  [0.9998777120836085, 0.9995482600825005]\n",
      "ppm_inv =  [66.60291782123463, 511.2397426303428]\n",
      "ppm_GD =  [122.28791639146141, 451.73991749947186]\n"
     ]
    }
   ],
   "source": [
    "# retrying, above is the oringinal one\n",
    "#try\n",
    "params_tru1 = [m.pi/6, m.pi/2+m.pi/6, m.pi/4, m.pi/2, m.pi/6]\n",
    "res = results(params_tru1, nit = 300, step = 1/200, N = 10000)\n",
    "\n",
    "print('tru = ', res[0])\n",
    "\n",
    "print('nj_unnorm = ', res[1])\n",
    "print('opt_inv = ', res[2])\n",
    "print('opt_GD = ', res[3])\n",
    "print('fid_inv = ', res[4])\n",
    "print('fid_GD = ', res[5])\n",
    "print('ppm_inv = ', res[6])\n",
    "print('ppm_GD = ', res[7])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# one saved result for params_tru1 = [m.pi/6, m.pi/2+m.pi/6, m.pi/4, m.pi/2, m.pi/6]\n",
    "tru =  [0.5235987755982988, 2.0943951023931953, 0.7853981633974483, 1.5707963267948966, 0.5235987755982988]  # same as [m.pi/6, m.pi/2+m.pi/6, m.pi/4, m.pi/2, m.pi/6]\n",
    "nj_unnorm =  [654, 558, 1169, 967, 2313, 521, 1260, 1152, 1406]\n",
    "opt_inv =  [0.5254370610538157, 1.030984356143689, 0.7670544953445808, -1.6068690955721805, (0.5346630644404312+2.5302074683784063e-17j)]\n",
    "opt_GD =  [0.5320294066145164, 1.0327066070674442, 0.7689496798867287, -1.6064174644839841, (0.5366920641304006+2.5302074683784063e-17j)]\n",
    "fid_inv =  [0.9999333970821788, 0.9994887602573697]\n",
    "fid_GD =  [0.9998777120836085, 0.9995482600825005]\n",
    "ppm_inv =  [66.60291782123463, 511.2397426303428]\n",
    "ppm_GD =  [122.28791639146141, 451.73991749947186] \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[2.620468715033219,\n",
       "   2.096494845467305,\n",
       "   3.9114857464288484,\n",
       "   1.4589030314243545,\n",
       "   0.5408658293314748],\n",
       "  [0.5307330358786534,\n",
       "   2.0624357350425906,\n",
       "   0.7728786246881507,\n",
       "   1.5799142276391531,\n",
       "   0.5169653275031013],\n",
       "  [2.610313065974753,\n",
       "   2.0650790496323403,\n",
       "   3.9417660898299878,\n",
       "   1.6401453721317285,\n",
       "   0.510908991325928],\n",
       "  [3.6531942178637054,\n",
       "   2.0773711313590955,\n",
       "   0.7944103700028262,\n",
       "   1.5921864340418774,\n",
       "   2.6174525639468436],\n",
       "  [0.5128119471599131,\n",
       "   2.0927995741456185,\n",
       "   0.7593622889818816,\n",
       "   1.5724015950617436,\n",
       "   2.596075597410029]],\n",
       " [[0.9999489291082766, 0.9976448536837781],\n",
       "  [0.9999194759454406, 0.998963965320031],\n",
       "  [0.9998997159677367, 0.9982714789693796],\n",
       "  [0.9998410605320175, 0.999626163265139],\n",
       "  [0.9997581684718029, 0.9999969720183342]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_tru1 = [m.pi/6, m.pi/2+m.pi/6, m.pi/4, m.pi/2, m.pi/6]   # degrees : 30, 120, 45, 90, 30\n",
    "gd_rand(params_tru = params_tru1, N = 10000, nit = 200, rat = 50, step = 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7474857108667748, 0.24734139200269095]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "martin_p =[ 0.5264971926438728, 2.621069253994012, 2.634918539482526 ,2.595433045991269, 0.760913797157194 ]\n",
    "p_tru= [0, m.pi/2, 0, 0, m.pi/4]\n",
    "\n",
    "fn.fid(p_tru, martin_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.164373601654298"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rough\n",
    "njn = [805, 1593, 836, 803, 1705, 851, 841, 1732, 834]\n",
    "[0,1.570796326794897, 0,0, 0.7853981633974483]\n",
    "\n",
    "ll = loss_new(vars = [0,1.570796326794897, 0,0, 0.7853981633974483], nj_unnorm = [805, 1593, 836, 803, 1705, 851, 841, 1732, 834], lossfn = 'old')\n",
    "ll\n",
    "kk = loss_new(vars = [0.5264971896014186 ,0.5205234056820061, 2.595433049161852 ,5.776511197655469, 2.331710128012299 ], nj_unnorm = [805, 1593, 836, 803, 1705, 851, 841, 1732, 834], lossfn = 'old')\n",
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prd: \n",
      " [0.0833, 0.0833, 0.1667, 0.0833, 0.0833, 0.1667, 0.0833, 0.0833, 0.1667]\n",
      "n: \n",
      " [0.0805, 0.1593, 0.0836, 0.0803, 0.1705, 0.0851, 0.0841, 0.1732, 0.0834]\n",
      "loss at params list:\n",
      "  [-0.2098741912766497, -0.15301927297010898, -0.41370135020720983, -0.21008140531385078, -0.1473593645221385, -0.4107368381752482, -0.20622986770986945, -0.14605057996172172, -0.4141006318352265]\n",
      "loss at params:\n",
      "  2.311153501972024\n"
     ]
    }
   ],
   "source": [
    "#rough\n",
    "prd = [round(i(0,1.570796326794897, 0,0, 0.7853981633974483),4) for i in pr_num]\n",
    "# print(sum(prd))\n",
    "print('prd: \\n', prd)\n",
    "n = [i/sum(njn) for i in njn]\n",
    "print('n: \\n', n)\n",
    "# th min loss\n",
    "loss_th_min = -sum([i*np.log(i) if i != 0 else 0 for i in n])\n",
    "\n",
    "#old loss\n",
    "lnl_p_list = [p * np.log(i) if i != 0 else 0 for i, p in zip(n, prd)]\n",
    "lnl_p = sum([abs(i) for i in lnl_p_list])\n",
    "print('loss at params list:\\n ', lnl_p_list)\n",
    "print('loss at params:\\n ', lnl_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec_loss:  2.310164245568836\n",
      "prd: \n",
      " [0.0833, 0.0833, 0.1667, 0.0833, 0.0833, 0.1667, 0.0833, 0.0833, 0.1667]\n",
      "collapse prob: \n",
      " [0.0805, 0.1593, 0.0836, 0.0803, 0.1705, 0.0851, 0.0841, 0.1732, 0.0834]\n"
     ]
    }
   ],
   "source": [
    "spec_params = [0,1.570796326794897, 0,0, 0.7853981633974483]\n",
    "njn = [805, 1593, 836, 803, 1705, 851, 841, 1732, 834]\n",
    "nn = [i/sum(njn) for i in njn]\n",
    "spec_loss = loss_new(vars = spec_params, nj_unnorm = njn, lossfn = 'old')\n",
    "print('spec_loss: ', spec_loss)\n",
    "\n",
    "#prob comparison at spec params\n",
    "prd = [round(i(*spec_params),4) for i in pr_num]\n",
    "print('prd: \\n', prd)\n",
    "print('collapse prob: \\n', nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr_spec_abstract: \n",
      " [0.0833333333333334, 0.0833333333333333, 0.166666666666667, 0.0833333333333332 + 1.7563329547635e-18*I, 0.0833333333333333 - 1.14422459262848e-49*I, 0.166666666666667 + 1.7563329547635e-18*I, 0.0833333333333332 - 3.90308845140291e-19*I, 0.0833333333333333 + 5.14884401085484e-49*I, 0.166666666666667 - 3.90308845140289e-19*I]\n"
     ]
    }
   ],
   "source": [
    "# again comparing the probabilities but this time using abstract ones generated directly from computer not using professor hillary's ones.\n",
    "\n",
    "rho_abs = fn.Creating_states(Abstract = 1)[2]        #abstract rho\n",
    "\n",
    "#abstract probs\n",
    "pr_abs = [np.trace(np.dot(fn.POVM_elts[i],rho_abs)) for i in range(9)]\n",
    "# pr_abs[1]\n",
    "pr_spec_abs = [i.subs({fn.theta1: spec_params[0], fn.theta2: spec_params[1], fn.phi1: spec_params[2], fn.phi2: spec_params[3], fn.alpha: spec_params[4]}) for i in pr_abs]\n",
    "\n",
    "print('pr_spec_abstract: \\n', pr_spec_abs)\n",
    "\n",
    "# Since the abstract probabilities are the same as Professor Hillary's probabilities but different from the collapsed probabilities, the error must be somewhere in the collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th_probs: \n",
      " [0.083333333333, 0.083333333333, 0.166666666667, 0.083333333333, 0.083333333333, 0.166666666667, 0.083333333333, 0.083333333333, 0.166666666667]\n",
      "collapses: \n",
      " [835, 833, 1625, 859, 881, 1650, 792, 825, 1700]\n"
     ]
    }
   ],
   "source": [
    "# checking if the error is in the num_experiment function\n",
    "\n",
    "expt = fn.num_experiment(params= [0,m.pi/2, 0,0, m.pi/4], N = 10000, an_pr= True)\n",
    "\n",
    "th_probs= expt[3]\n",
    "collapses = expt[4]\n",
    "collapse_prob = [i/sum(collapses) for i in collapses]\n",
    "\n",
    "print('th_probs: \\n', th_probs)\n",
    "print('collapses: \\n', collapses)\n",
    "\n",
    "# the collapses are correct, the error is in camillas probs..Now starting over using a fresh example to trace where and why does the loss exceed the minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th max loss:  2.1377889507324106\n",
      "[(3.058975036141985, [0.07583647691419862, 3.0845492930528158, 2.554248964262986, 1.5478842657524106, 1.836657100525946]), (2.954898917583247, [1.5487356722524563, 1.4829360691778117, 0.8778573743716248, 1.3783192693168582, 2.2772220070995925]), (2.9057594156177355, [2.321815929385002, 3.0911693488166883, 0.1090113937111211, 1.145769653231649, 1.6769127917084168]), (2.878629547528605, [2.3052475650573574, 0.028609504657239427, 1.0236013879089865, 0.2347942431403365, 1.735105019665889]), (2.7704404843407007, [0.07923659679156346, 1.4696350150740096, 1.4315467330035465, 1.3287969705329492, 1.5832193353442658]), (2.6970623297891327, [0.7629700074232287, 0.8181669811855173, 0.342799465870813, 0.34022433283097975, 2.620180199046174]), (2.688638330630604, [2.4800239245082847, 0.08538550503363386, 1.7170642751022025, 0.7128891978922417, 1.7449483144863052]), (2.6883576996197482, [0.11701345146873242, 3.0059184508813828, 2.297166243252706, 2.8688048637349963, 2.4262394652273778]), (2.6882030065608435, [0.9462599455194848, 2.9760101599547424, 0.20351593694514022, 0.4889159745830224, 2.9920351179693054]), (2.6736656094174505, [2.3329834521006334, 0.518419166835961, 3.0281485069857546, 2.7977386582592207, 2.8077171390512463])]\n"
     ]
    }
   ],
   "source": [
    "#writing a loop to filter out the highest loss iteratively\n",
    "params_tru = [0, m.pi/2, 0, 0, m.pi/4]\n",
    "n_fresh =  [842, 854, 1615, 799, 800, 1714, 892, 813, 1671]\n",
    "n_fresh_norm = [i/sum(n_fresh) for i in n_fresh]\n",
    "th_loss_max = -sum([i*np.log(i) if i != 0 else 0 for i in n_fresh_norm])\n",
    "params_list = []\n",
    "loss_list = []\n",
    "for i in range(100):\n",
    "    params_i = [rand.uniform(0,m.pi) for j in range(5)]\n",
    "    loss = loss_new(vars = params_i, nj_unnorm = n_fresh, lossfn = 'old')\n",
    "    params_list.append(params_i)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "# zip the lists, pick max\n",
    "results_list = zip(loss_list, params_list)\n",
    "results_list = sorted(results_list, key = lambda x: x[0], reverse = True)\n",
    "print('th max loss: ', th_loss_max)\n",
    "print(results_list[:10])\n",
    "# from here we see that at parameters [0.7067015919596139, 0.876436876817383, 1.6730930859371373, 0.26594776402832554, 0.30328983213952054] ,the loss is 2.3218 instead of the minimum possible 2.1377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_nice:  2.7996275431571207\n",
      "pr_an: \n",
      " [0.00070640100534, 0.02053862432267, 0.0288630315947, 0.22834151721771, 0.11438869642226, 0.13221575789868, 0.22834151721771, 0.11438869642226, 0.13221575789868]\n",
      "collapse probs: \n",
      " [0.0842, 0.0854, 0.1615, 0.0799, 0.08, 0.1714, 0.0892, 0.0813, 0.1671]\n",
      "pr_spec_abstract: \n",
      " [-0.0745517784 + 0.0803511791*I, -0.0086691976 - 0.135608464*I, 0.1468419221 + 0.0266642165*I, -0.0212539276 - 0.0343654989*I, -0.0152000948 + 0.0507111149*I, 0.0473994576 + 0.0095710994*I, -0.0212539276 - 0.0343654989*I, -0.0152000948 + 0.0507111149*I, 0.0473994576 + 0.0095710994*I]\n",
      "1.0000000000000102\n"
     ]
    }
   ],
   "source": [
    "# Now, finding where or how the loss is increasing. Probably there's a mismatch somehow in the collapsed probabilities and the theoretical probabilities. \n",
    "# Checking for that now at the special parameters found above.\n",
    "\n",
    "spec_params = [0.7067015919596139, 0.876436876817383, 1.6730930859371373, 0.26594776402832554, 0.30328983213952054]\n",
    "spec_params_nicer = [0, 1, 0, 0, m.pi/2]\n",
    "loss_nice = loss_new(vars = spec_params_nicer, nj_unnorm = n_fresh, lossfn = 'old')\n",
    "print('loss_nice: ', loss_nice)\n",
    "# n_fresh =  [842, 854, 1615, 799, 800, 1714, 892, 813, 1671]\n",
    "\n",
    "pr_an = [round(i(*spec_params_nicer),14) for i in pr_num]\n",
    "print('pr_an: \\n', pr_an)\n",
    "print('collapse probs: \\n', n_fresh_norm)\n",
    "\n",
    "pr_spec_abs_sym = [i.subs({fn.theta1: spec_params[0], fn.theta2: spec_params[1], fn.phi1: spec_params[2], fn.phi2: spec_params[3], fn.alpha: spec_params[4]}) for i in pr_abs]\n",
    "pr_spec_abs = [round(i.evalf(),10) for i in pr_spec_abs_sym]\n",
    "print('pr_spec_abstract: \\n', pr_spec_abs)\n",
    "#It is crazy that the probabilities are coming out to be complex using the abstract method Something must be wrong in Basis arrangement...\n",
    "\n",
    "print(sum(pr_an))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual_loss_list: \n",
      " [-0.6108985742780509, -0.3318172636623165, -0.5725487803361318, -0.1180053397672549, -0.1734522409932759, -0.3467970756382234, -0.13174062962752361, -0.17627083990941664, -0.338096798944849]\n",
      "manual_loss:  2.799627543157043\n",
      "loss at nice params:  2.279530525594202\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "manual_loss_list = [n*np.log(p) if p!= 0 else 0 for n,p in zip(n_fresh_norm, pr_an)]\n",
    "manual_loss = -sum(manual_loss_list)\n",
    "print('manual_loss_list: \\n', manual_loss_list)\n",
    "print('manual_loss: ', manual_loss)\n",
    "print('loss at nice params: ', loss_nice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/8__h5b611fz3d61c954t7_540000gn/T/ipykernel_58481/4224283144.py:17: RuntimeWarning: invalid value encountered in log\n",
      "  lnl_p = [i * np.log(p) if p != 0 else 0 for i, p in zip(n, p)]                          # manually putting zero for cases of p_i*log(n_i) , if any n_i are 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " message: Optimization terminated successfully\n",
       " success: True\n",
       "  status: 0\n",
       "     fun: 2.1377889569311237\n",
       "       x: [ 8.421e-02  8.542e-02  1.615e-01  7.991e-02  8.000e-02\n",
       "            1.714e-01  8.920e-02  8.128e-02  1.671e-01]\n",
       "     nit: 7\n",
       "     jac: [-1.000e+00 -1.001e+00 -1.000e+00 -1.001e+00 -1.000e+00\n",
       "           -9.998e-01 -9.995e-01 -9.988e-01 -1.000e+00]\n",
       "    nfev: 77\n",
       "    njev: 7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opt_p = GD_new([0, m.pi/4, 0, 1, .9] , nit = 20, step = 1/10, nj_unnorm = [0.0842, 0.0854, 0.1615, 0.0799, 0.08, 0.1714, 0.0892, 0.0813, 0.1671] , N = 10000)\n",
    "\n",
    "# new method to simply minimize the loss function directly without writing an explicit algorithm for it.\n",
    "\n",
    "# first trying using the probabilities as variables instead of the parameters and seeing how accurate we can get and then later on trying with the parameters\n",
    "\n",
    "# objective fn\n",
    "# loss_new(vars, nj_unnorm = [842, 854,1615,799,800,1714,892,813,1671], lossfn ='old')\n",
    "\n",
    "\n",
    "# define function for cross entropy of list n and list p\n",
    "\n",
    "nju= [842, 854,1615,799,800,1714,892,813,1671]\n",
    "njn = [i/sum(nju) for i in nju]\n",
    "# njn = [0.0842, 0.0854, 0.1615, 0.0799, 0.08, 0.1714, 0.0892, 0.0813, 0.1671]\n",
    "def loss_t(p, n = [0.0842, 0.0854, 0.1615, 0.0799, 0.08, 0.1714, 0.0892, 0.0813, 0.1671]):\n",
    "    lnl_p = [i * np.log(p) if p != 0 else 0 for i, p in zip(n, p)]                          # manually putting zero for cases of p_i*log(n_i) , if any n_i are 0.\n",
    "    loss = sum([abs(i) for i in lnl_p])\n",
    "    return loss\n",
    "    \n",
    "\n",
    "\n",
    "def constrain_p_sum(p):\n",
    "    return (sum(p) - 1)\n",
    "\n",
    "\n",
    "cons = {'type': 'eq', 'fun': constrain_p_sum}\n",
    "ig = [1/9 for i in range(9)]\n",
    "sol = GDlib(loss_t, ig, method='SLSQP', constraints=cons)\n",
    "\n",
    "sol\n",
    "# Result: . It worked amazing, the result was fantastic, it was matching up to like eighth decimal and the probabilities were also very very close. Although the fidelities could not be found just using these for that we need parameters that is the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " message: Optimization terminated successfully\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 0.36169035943175454\n",
      "       x: [ 1.314e-01  1.358e+00  1.495e+00  1.254e+00  7.609e-01]\n",
      "     nit: 57\n",
      "     jac: [-2.096e-02 -1.718e-01  2.293e-03 -8.954e-03  3.853e-01]\n",
      "    nfev: 409\n",
      "    njev: 57\n",
      "fid:  [0.9828217580281529, 0.9555014482984382]\n"
     ]
    }
   ],
   "source": [
    "# Now, trying to find the parameters \n",
    "# first the inequality constraints for all of the five variables for them to remain in between 0 and pi by 1 for the first four variables and 0 and pi over 2 for the fifth variable\n",
    "\n",
    "def c_1_g(params):\n",
    "    return (m.pi + 1e-12 - params[0])               # 1e-12 is added to avoid the floating point error\n",
    "\n",
    "def c_2_g(params):\n",
    "    return (m.pi + 1e-12 - params[1])\n",
    "\n",
    "def c_3_g(params):\n",
    "    return (m.pi + 1e-12 - params[2])\n",
    "\n",
    "def c_4_g(params):\n",
    "    return (m.pi + 1e-12 - params[3])\n",
    "\n",
    "def c_5_g(params):\n",
    "    return (m.pi/2 + 1e-12 - params[4])\n",
    "\n",
    "def c_1_l(params):\n",
    "    return (params[0])\n",
    "\n",
    "def c_2_l(params):\n",
    "    return (params[1]+ 1e-12)\n",
    "\n",
    "def c_3_l(params):\n",
    "    return (params[2]+ 1e-12)\n",
    "\n",
    "def c_4_l(params):  \n",
    "    return (params[3]+ 1e-12)\n",
    "\n",
    "def c_5_l(params):\n",
    "    return (params[4]+ 1e-12)\n",
    "\n",
    "\n",
    "cons_ineq = ({'type': 'ineq', 'fun': c_1_g}, {'type': 'ineq', 'fun': c_2_g}, {'type': 'ineq', 'fun': c_3_g}, {'type': 'ineq', 'fun': c_4_g}, {'type': 'ineq', 'fun': c_5_g}, {'type': 'ineq', 'fun': c_1_l}, {'type': 'ineq', 'fun': c_2_l}, {'type': 'ineq', 'fun': c_3_l}, {'type': 'ineq', 'fun': c_4_l}, {'type': 'ineq', 'fun': c_5_l})\n",
    "ig = [m.pi/12, m.pi/3, 1, m.pi/4, m.pi/8]\n",
    "\n",
    "sol = GDlib(loss_new, ig, method='SLSQP', constraints=cons_ineq, args = ([0, m.pi/2, 0, 0, m.pi/4], 10000, xxnn, 'new'))\n",
    "\n",
    "# sol = GDlib(loss_new, ig, method='SLSQP', args = ([0, m.pi/2, 0, 0, m.pi/4], 10000, nju, 'old'))\n",
    "print(sol)\n",
    "print('fid: ', fn.fid(sol.x, [0, m.pi/2, 0, 0, m.pi/4]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1188, 1304, 0, 1258, 1250, 1216, 1248, 1299, 1237]\n",
      "min 2.079019206430704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0794782168015233"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_inv = [45.413971053989975, 44.97208496779245, 0.5324424926602901, -179.92245596910786, (44.65140134482599-7.67504195354202e-17j)]\n",
    "p_inv_rad = [i*(m.pi/180) for i in p_inv]\n",
    "\n",
    "# Now, trying to find the corresponding cross entropy or the loss function for the inversion parameters\n",
    "\n",
    "xxn = fn.num_experiment(params= p_inv_rad, N = 10000, an_pr= True)[4]\n",
    "xxnn = [i/sum(xxn) for i in xxn]\n",
    "print(xxn)\n",
    "inv_loss = loss_new(p_inv_rad, nj_unnorm = xxn , lossfn ='old')\n",
    "\n",
    "print('min', loss_t(xxnn, xxnn))\n",
    "inv_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/8__h5b611fz3d61c954t7_540000gp/T/ipykernel_36110/4224283144.py:17: RuntimeWarning: invalid value encountered in log\n",
      "  lnl_p = [i * np.log(p) if p != 0 else 0 for i, p in zip(n, p)]                          # manually putting zero for cases of p_i*log(n_i) , if any n_i are 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " message: Optimization terminated successfully\n",
       " success: True\n",
       "  status: 0\n",
       "     fun: 2.079029317672689\n",
       "       x: [ 1.195e-01  1.294e-01  4.289e-07  1.260e-01  1.253e-01\n",
       "            1.217e-01  1.251e-01  1.291e-01  1.239e-01]\n",
       "     nit: 21\n",
       "     jac: [-9.941e-01 -1.008e+00  0.000e+00 -9.980e-01 -9.978e-01\n",
       "           -9.991e-01 -9.978e-01 -1.006e+00 -9.982e-01]\n",
       "    nfev: 297\n",
       "    njev: 21"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxn = [1188, 1304, 0, 1258, 1250, 1216, 1248, 1299, 1237]\n",
    "xxnn = [i/sum(xxn) for i in xxn]\n",
    "\n",
    "# Now, trying to optimize using the probability parameters method to compare the optimization results with the inversion method\n",
    "\n",
    "ig = [1/9 for i in range(9)]\n",
    "sol = GDlib(loss_t, ig, method='SLSQP', constraints=cons, args= (xxnn))\n",
    "\n",
    "sol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/pranjal/Desktop/Python/Research_python/Pair_disc_code/MLE_final_code_24Apr.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pranjal/Desktop/Python/Research_python/Pair_disc_code/MLE_final_code_24Apr.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m nju1 \u001b[39m=\u001b[39m [i\u001b[39m/\u001b[39m\u001b[39msum\u001b[39m(njn1) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m njn1]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pranjal/Desktop/Python/Research_python/Pair_disc_code/MLE_final_code_24Apr.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m nju2 \u001b[39m=\u001b[39m [i\u001b[39m/\u001b[39m\u001b[39msum\u001b[39m(njn2) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m njn2]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pranjal/Desktop/Python/Research_python/Pair_disc_code/MLE_final_code_24Apr.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss1martin \u001b[39m=\u001b[39m loss_new(martin_opt_case1, nj_unnorm \u001b[39m=\u001b[39m njn1, lossfn \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mold\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pranjal/Desktop/Python/Research_python/Pair_disc_code/MLE_final_code_24Apr.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss2martin \u001b[39m=\u001b[39m loss_new(martin_opt_case2, nj_unnorm \u001b[39m=\u001b[39m njn2, lossfn \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mold\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pranjal/Desktop/Python/Research_python/Pair_disc_code/MLE_final_code_24Apr.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(loss1martin, loss2martin)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_new' is not defined"
     ]
    }
   ],
   "source": [
    "martin_opt_case1 = [1.5486207416093, 3.1274442725918, 4.7643344242595, 0.21847560771103, 0.77864789419447]\n",
    "martin_opt_case2 = [2.6072033366248, 4.1823366657766, 3.9196739422848, 1.5574885302675, 0.54017337241981]\n",
    "tru_2 = [0.5235987755982988, 2.0943951023931953, 0.7853981633974483, 1.5707963267948966, 0.5235987755982988] \n",
    "njn1 = [842, 854,1615,799,800,1714,892,813,1671]\n",
    "njn2 = [654, 558, 1169, 967,  2313, 521,  1260, 1152,1406]\n",
    "\n",
    "nju1 = [i/sum(njn1) for i in njn1]\n",
    "nju2 = [i/sum(njn2) for i in njn2]\n",
    "\n",
    "loss1martin = loss_new(martin_opt_case1, nj_unnorm = njn1, lossfn = 'old')\n",
    "loss2martin = loss_new(martin_opt_case2, nj_unnorm = njn2, lossfn = 'old')\n",
    "\n",
    "print(loss1martin, loss2martin)\n",
    "print([i for i in martin_opt_case1[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88.72943256063249, 179.1893574818719, 272.97625469896036, 12.517730248397843, 44.61323806409221]\n",
      "[149.3817475210271, 239.63023945181598, 224.58077396032405, 89.23751942436132, 30.94965444500354]\n",
      "[29.999999999999996, 119.99999999999999, 45.0, 90.0, 29.999999999999996]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9995083240245686, 0.9997998366716809]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# degrees\n",
    "print([i*(180/m.pi) for i in martin_opt_case1])\n",
    "print([i*(180/m.pi) for i in martin_opt_case2])\n",
    "print([i*(180/m.pi) for i in tru_2])\n",
    "\n",
    "# print(m.pi/4)\n",
    "fn.fid(martin_opt_case1, [m.pi/2, 0, 0, 0, m.pi/4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0945855608529884"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_t(nju2, nju2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0654, 0.0558, 0.1169, 0.0967, 0.2313, 0.0521, 0.126, 0.1152, 0.1406]\n",
      "[0.06793312233662688, 0.05364199613778085, 0.11887112345186476, 0.16988019120071673, 0.2546207101352122, 0.053578937935631316, 0.051837943260331074, 0.09371230486351896, 0.1359236706783202]\n"
     ]
    }
   ],
   "source": [
    "pr_martin2 = [i(*martin_opt_case2) for i in pr_num]\n",
    "\n",
    "print(nju2)\n",
    "print(pr_martin2)\n",
    "\n",
    "\n",
    "# loss_t(pr_martin2, nju2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.860579819920731 \n",
      " 0.5055784740593927\n",
      "0.5093155932672808 \n",
      " 0.8627806248217307\n",
      "0.7355112613814505 \n",
      " 0.26448873861854955\n",
      "\n",
      " cos theta_j 0.860579819920731 0.5055784740593927\n",
      "sin theta_j 0.5093155932672808 0.8627806248217307\n",
      " cos^2, sin^2 alpha 0.7355112613814505 0.26448873861854955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.094972152688196"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_martin = [0.5343893173506,1.0407440166693,0.7780813019974701,4.6990812421347,2.6014192818704]\n",
    "\n",
    "#print above but in a single line each\n",
    "print('\\n cos theta_j',m.cos(check_martin[0]), m.cos(check_martin[1]))\n",
    "print('sin theta_j',m.sin(check_martin[0]), m.sin(check_martin[1]))\n",
    "print(' cos^2, sin^2 alpha',m.cos(check_martin[4])**2, m.sin(check_martin[4])**2)\n",
    "\n",
    "# probs\n",
    "\n",
    "pr_martin2_check = [i(*check_martin) for i in pr_num]\n",
    "pr_martin2_check\n",
    "\n",
    "Loss_check = loss_t(pr_martin2_check, nju2)\n",
    "Loss_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fidelity function using parameters\n",
    "def fid(params1 = [0,m.pi/2,0,0,m.pi/4], params2 = [0,m.pi/2,0,0,m.pi/4], coeff_mode = False ):\n",
    "    #if simply the coeffs 4-lists are given\n",
    "    if coeff_mode == True:\n",
    "        fid0 = (abs(sum([np.conj(params1[i])*params2[i] for i in range(0,2)])))**2\n",
    "        fid1 = (abs(sum([np.conj(params1[i])*params2[i] for i in range(2,4)])))**2\n",
    "    else:               # generating the qm states from the parameters to find the fidelities\n",
    "        states1 = fn.Creating_states(params1)[0]\n",
    "        states2 = fn.Creating_states(params2)[0]\n",
    "        # Calculate the fidelities between the corresponding states\n",
    "        fid0_sym = (abs(states1[0].H * states2[0])**2).evalf()\n",
    "        fid1_sym = (abs(states1[1].H * states2[1])**2).evalf()\n",
    "        # Extract the real part of the fidelity values\n",
    "        fid0 = complex(fid0_sym[0]).real\n",
    "        fid1 = complex(fid1_sym[0]).real\n",
    "    return [fid0, fid1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9986095076327963, 0.9842604563694797]\n",
      "[0.9987137132359978, 0.9999592342419349]\n"
     ]
    }
   ],
   "source": [
    "tru_deg = [29.999999999999996, 59.99999999999999, 29.999999999999996, 0.0, 29.999999999999996]\n",
    "params_inv_deg =  [29.638608229696608, 62.537229284179226, 25.117537308310606, -16.030328746515185, (28.435771189456087+1.6630221342149307e-15j)]\n",
    "params_opt = [(62.052359057321794+0j), (30.03131138748132+0j), (-0.2614468121347648+0j), (29.15852708313148+0j), (59.32744200079733+2.9025211775593585e-17j)]\n",
    "\n",
    "# in radians\n",
    "tru = [i*(m.pi/180) for i in tru_deg]\n",
    "params_inv = [i*(m.pi/180) for i in params_inv_deg]\n",
    "params_opt = [i*(m.pi/180) for i in params_opt]\n",
    "\n",
    "# fid([0, m.pi/2, 0, 0, m.pi/4], [0, m.pi/2, 0, 0, m.pi/4])\n",
    "print(fid(tru, params_inv))\n",
    "print(fid([m.pi/3 ,m.pi/6, 0, m.pi/6, m.pi/6], params_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.999873407960869, 0.9999249028120026]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[126.59203913101268, 75.09718799736387]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(fn.fid(check_martin, tru_2))\n",
    "#ppm error\n",
    "ppm = [(1-i)*1e6 for i in fn.fid(check_martin, tru_2)]\n",
    "ppm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Eigenvalues:\n",
      " [1. 4. 0.]\n",
      "Original Eigenvectors:\n",
      " [[ 1.          0.5547002  -0.2981424 ]\n",
      " [ 0.          0.83205029 -0.74535599]\n",
      " [ 0.          0.          0.59628479]]\n",
      "[[ 0.98  1.98  2.98]\n",
      " [ 0.    3.98  4.98]\n",
      " [-0.02  0.   -0.02]]\n",
      "\n",
      "Perturbed Eigenvalues:\n",
      " [ 0.98623402 -0.00972338  3.96348936]\n",
      "Perturbed Eigenvectors:\n",
      " [[ 0.99925672 -0.3058711   0.55103826]\n",
      " [ 0.03303844 -0.74302732  0.83447539]\n",
      " [-0.01986132  0.5952758  -0.00276661]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Original matrix\n",
    "A = np.array([[1, 2, 3],\n",
    "              [0, 4, 5],\n",
    "              [0, 0, 0]])\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigvals, eigvecs = np.linalg.eig(A)\n",
    "\n",
    "print(\"Original Eigenvalues:\\n\", eigvals)\n",
    "print(\"Original Eigenvectors:\\n\", eigvecs)\n",
    "\n",
    "# Perturbed matrix\n",
    "perturbation = 0.02\n",
    "A_perturbed = A - perturbation * np.array([[1,1,1],[0,1,1],[1,0,1]])\n",
    "\n",
    "print(A_perturbed)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors of the perturbed matrix\n",
    "eigvals_perturbed, eigvecs_perturbed = np.linalg.eig(A_perturbed)\n",
    "\n",
    "print(\"\\nPerturbed Eigenvalues:\\n\", eigvals_perturbed)\n",
    "print(\"Perturbed Eigenvectors:\\n\", eigvecs_perturbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues (SymPy): {1.74575537758787e-11 - 4.61562975474726e-33*I: 1, 0.242082882894236 + 1.86389228904848e-34*I: 1, 0.757917117088306 + 4.88059210183399e-33*I: 1}\n",
      "Eigenvector for eigenvalue 1.74575537758787e-11 - 4.61562975474726e-33*I: Matrix([[-0.50855948606321 + 0.421304472669645*I], [-0.0616986117254196 + 0.657513110795123*I], [-0.167024729805028 + 0.315978354513266*I]])\n",
      "Eigenvector for eigenvalue 0.242082882894236 + 1.86389228904848e-34*I: Matrix([[-0.477883330600306 - 0.339811179657743*I], [0.549940977995031 + 0.203492103551704*I], [0.261163337259709 - 0.494070319184744*I]])\n",
      "Eigenvector for eigenvalue 0.757917117088306 + 4.88059210183399e-33*I: Matrix([[-0.464897140471242 + 0.0624205394025948*I], [-0.111854368854807 - 0.455537347673867*I], [0.618324878398021 + 0.42145343139993*I]])\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define the symbolic matrix\n",
    "rho = sp.Matrix([[0.25 + 0.00000000e+00j, -0.0625-1.87500000e-01j, -0.1875+9.91116524e-02j],\n",
    "                 [-0.0625+1.87500000e-01j, 0.25 - 1.58794886e-34j, -0.1875-9.91116524e-02j],\n",
    "                 [-0.1875-9.91116524e-02j, -0.1875+9.91116524e-02j, 0.5   +5.02387877e-35j]])\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues_sympy, eigenvectors_sympy = rho.eigenvals(), rho.eigenvects()\n",
    "\n",
    "# Print the eigenvalues\n",
    "print(\"Eigenvalues (SymPy):\", eigenvalues_sympy)\n",
    "\n",
    "# Extract and print the eigenvectors\n",
    "for eigenvalue, multiplicity, vectors in eigenvectors_sympy:\n",
    "    for vector in vectors:\n",
    "        print(f\"Eigenvector for eigenvalue {eigenvalue}: {vector}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
